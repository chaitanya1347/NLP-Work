{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67975,"databundleVersionId":7564075,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nimport ast\nfrom collections import defaultdict\n\n# Input data files are available in the read-only                       \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/assignment-1-nlp/train.csv') # loading training data\ndata = []\nfor index, row in tqdm(df.iterrows()):\n    data.append(ast.literal_eval(row['tagged_sentence'])) # changing data-type of entries from 'str' to 'list'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:16.703918Z","iopub.execute_input":"2024-02-10T14:12:16.704356Z","iopub.status.idle":"2024-02-10T14:12:30.715268Z","shell.execute_reply.started":"2024-02-10T14:12:16.704320Z","shell.execute_reply":"2024-02-10T14:12:30.714074Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/assignment-1-nlp/test_small.csv') # loading test data\ntest_data = {} \nfor index, row in tqdm(df.iterrows()):\n    test_data[row['id']] = ast.literal_eval(row['untagged_sentence']) # changing data-type of entries from 'str' to 'list'","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:30.718141Z","iopub.execute_input":"2024-02-10T14:12:30.718909Z","iopub.status.idle":"2024-02-10T14:12:31.333581Z","shell.execute_reply.started":"2024-02-10T14:12:30.718859Z","shell.execute_reply":"2024-02-10T14:12:31.332427Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_data(sentence_index):\n    '''\n        Input : 'sentence_index' (int) -> index of a sentence in training data\n        Output: None\n    '''\n    sentence = data[sentence_index]\n    print(\"TOKEN -> TAG\")\n    print('...')\n    for token, tag in sentence:\n        print(token, '>', tag)\nsentence_index = random.choice(range(len(data)))\ndisplay_data(sentence_index)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:31.335030Z","iopub.execute_input":"2024-02-10T14:12:31.336115Z","iopub.status.idle":"2024-02-10T14:12:31.344262Z","shell.execute_reply.started":"2024-02-10T14:12:31.336070Z","shell.execute_reply":"2024-02-10T14:12:31.342797Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cell to show the frequency of each distinct (slack or native) present in the training data\nfrom collections import Counter\ndistinct_tags = []\nword_tags = []\ndef store_tags():\n    \n    global distinct_tags\n    global word_tags\n    \n    for sent in data:\n        word_tags.append(('START','START'))\n        for words, tag in sent:\n            word_tags.extend([(tag, words)])\n        word_tags.append(('END','END'))\n    \nstore_tags()\ntags=[]\nfor tag, words in word_tags:\n    tags.append(tag)\ndistinct_tags=list(set(tags))\ncount_tags = {}\nfor tag, count in Counter(tags).items():\n    count_tags[tag] = count","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:31.347099Z","iopub.execute_input":"2024-02-10T14:12:31.348089Z","iopub.status.idle":"2024-02-10T14:12:32.104120Z","shell.execute_reply.started":"2024-02-10T14:12:31.348041Z","shell.execute_reply":"2024-02-10T14:12:32.102837Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_list = [tag for sentence in data for word,tag in sentence ]\nprint(len(data))\nprint((tag_list[:10]))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:32.105407Z","iopub.execute_input":"2024-02-10T14:12:32.105781Z","iopub.status.idle":"2024-02-10T14:12:32.241489Z","shell.execute_reply.started":"2024-02-10T14:12:32.105747Z","shell.execute_reply":"2024-02-10T14:12:32.240299Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_tags = {tag for sentence in data for word,tag in sentence}\nprint(unique_tags)\nprint(len(unique_tags))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:12:32.242775Z","iopub.execute_input":"2024-02-10T14:12:32.243190Z","iopub.status.idle":"2024-02-10T14:12:32.327209Z","shell.execute_reply.started":"2024-02-10T14:12:32.243156Z","shell.execute_reply":"2024-02-10T14:12:32.325962Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_counts = {}   \nfor sentence in data:\n    for word , tag in sentence:\n        tag_counts[tag] = tag_counts.get(tag, 0) + 1\nprint(tag_counts)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T07:07:12.929769Z","iopub.execute_input":"2024-02-10T07:07:12.930075Z","iopub.status.idle":"2024-02-10T07:07:13.235586Z","shell.execute_reply.started":"2024-02-10T07:07:12.930048Z","shell.execute_reply":"2024-02-10T07:07:13.234183Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"default_prob = 1e-10","metadata":{"execution":{"iopub.status.busy":"2024-02-09T18:24:41.450847Z","iopub.execute_input":"2024-02-09T18:24:41.451867Z","iopub.status.idle":"2024-02-09T18:24:41.457157Z","shell.execute_reply.started":"2024-02-09T18:24:41.451820Z","shell.execute_reply":"2024-02-09T18:24:41.456073Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **HMM Model**","metadata":{}},{"cell_type":"code","source":"def calculate_start_probabilities(corpus):\n    start_prob = {}\n    for tag in unique_tags:\n        start_prob[tag] = default_prob\n    for sentence in corpus:\n        if sentence:  # Check if the sentence is not empty\n            start_prob[sentence[0][1]] += 1\n    total_sentences = sum(start_prob.values())\n    start_prob = {state: count / total_sentences for state, count in start_prob.items()}\n    return (start_prob)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:33.950408Z","iopub.execute_input":"2024-02-09T17:03:33.950732Z","iopub.status.idle":"2024-02-09T17:03:34.012859Z","shell.execute_reply.started":"2024-02-09T17:03:33.950702Z","shell.execute_reply":"2024-02-09T17:03:34.011482Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(calculate_start_probabilities(data))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T19:37:37.062832Z","iopub.execute_input":"2024-02-09T19:37:37.063374Z","iopub.status.idle":"2024-02-09T19:37:37.106507Z","shell.execute_reply.started":"2024-02-09T19:37:37.063334Z","shell.execute_reply":"2024-02-09T19:37:37.105353Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#computing transition probability\ndef transition_proba(training_corpus):\n    transition = {}\n    for sentence in training_corpus:\n        for index in range(0, len(sentence) - 1):\n            current = sentence[index][1]\n            next_elem = sentence[index + 1][1]\n            if(current not in transition):\n                transition[current] = {}\n            if(next_elem not in transition[current]):\n                transition[current][next_elem] = 0\n                \n            transition[current][next_elem] += 1\n            \n    for curr_state in transition:\n        total_transitions = sum(transition[curr_state].values())\n        for next_state,count in transition[curr_state].items():\n            transition[curr_state][next_state] = count / total_transitions\n            \n    return transition","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:34.063267Z","iopub.execute_input":"2024-02-09T17:03:34.063653Z","iopub.status.idle":"2024-02-09T17:03:34.072357Z","shell.execute_reply.started":"2024-02-09T17:03:34.063622Z","shell.execute_reply":"2024-02-09T17:03:34.070774Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(transition_proba(data))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T19:45:38.120712Z","iopub.execute_input":"2024-02-09T19:45:38.121191Z","iopub.status.idle":"2024-02-09T19:45:38.573832Z","shell.execute_reply.started":"2024-02-09T19:45:38.121157Z","shell.execute_reply":"2024-02-09T19:45:38.572417Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_emission_probabilities(corpus):\n    \n    emission_prob = {}\n    for sentence in corpus:\n        for word, state in sentence:\n            if(state not in emission_prob):\n                emission_prob[state] = {}\n            if(word not in emission_prob[state]):\n                emission_prob[state][word] = 0\n    \n            emission_prob[state][word] += 1\n\n    for state in emission_prob:\n        total_emissions = sum(emission_prob[state].values())\n        emission_prob[state] = {word: count / total_emissions\n                                for word, count in emission_prob[state].items()}\n        \n    return (emission_prob)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T07:07:13.685946Z","iopub.status.idle":"2024-02-10T07:07:13.686327Z","shell.execute_reply.started":"2024-02-10T07:07:13.686140Z","shell.execute_reply":"2024-02-10T07:07:13.686155Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"something = (calculate_emission_probabilities(data))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T07:07:13.687125Z","iopub.status.idle":"2024-02-10T07:07:13.687394Z","shell.execute_reply.started":"2024-02-10T07:07:13.687261Z","shell.execute_reply":"2024-02-10T07:07:13.687274Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states = list(unique_tags)\nstart_prob = calculate_start_probabilities(data)\ntransition_probs = transition_proba(data)\nemission_probs = calculate_emission_probabilities(data)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:34.982360Z","iopub.execute_input":"2024-02-09T17:03:34.982671Z","iopub.status.idle":"2024-02-09T17:03:35.890879Z","shell.execute_reply.started":"2024-02-09T17:03:34.982644Z","shell.execute_reply":"2024-02-09T17:03:35.889750Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"det = ['a','A','an','the','The','An']","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:35.892313Z","iopub.execute_input":"2024-02-09T17:03:35.893009Z","iopub.status.idle":"2024-02-09T17:03:35.899149Z","shell.execute_reply.started":"2024-02-09T17:03:35.892969Z","shell.execute_reply":"2024-02-09T17:03:35.897905Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Viterbi Algorithm**","metadata":{}},{"cell_type":"code","source":"def viterbi_algorithm(sentence):\n    n = len(sentence)\n    m = len(states)\n    \n    dp = np.zeros((n, m))\n    backpointer = np.zeros((n, m), dtype=int)\n    \n    # Initialization step\n    for i in range(m):\n        word = sentence[0]\n        dp[0][i] = start_prob.get(states[i], default_prob) * emission_probs[states[i]].get(word, default_prob)\n    \n    # Recursion step\n    for t in range(1, n):\n        for j in range(m):\n            word = sentence[t]\n            max_prob = 0\n            max_index = 0\n\n            for i in range(m):\n                transition_prob_ij = transition_probs.get(states[i], default_prob).get(states[j], default_prob)\n                prob = dp[t-1][i] * transition_prob_ij * emission_probs[states[j]].get(word, default_prob)\n\n                if prob > max_prob:\n                    max_prob = prob\n                    max_index = i\n            \n            dp[t][j] = max_prob\n            backpointer[t][j] = max_index\n\n    # Backtrack to find the best sequence of tags\n    best_sequence = [0] * n\n    best_index = np.argmax(dp[n-1, :])\n\n    for t in range(n-1, -1, -1):\n        best_sequence[t] = states[best_index]\n        best_index = backpointer[t][best_index]\n\n    return best_sequence","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:35.900453Z","iopub.execute_input":"2024-02-09T17:03:35.900766Z","iopub.status.idle":"2024-02-09T17:03:35.963525Z","shell.execute_reply.started":"2024-02-09T17:03:35.900738Z","shell.execute_reply":"2024-02-09T17:03:35.962160Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hmm_tagger_util(sent_id,untagged_sentence):\n    predicted_tags = viterbi_algorithm(untagged_sentence)\n    tagged_sentence = []\n    for index ,word in enumerate(list(untagged_sentence)):\n        tagged_sentence.append((word, predicted_tags[index]))\n    store_submission(sent_id, tagged_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:35.965333Z","iopub.execute_input":"2024-02-09T17:03:35.966040Z","iopub.status.idle":"2024-02-09T17:03:35.975166Z","shell.execute_reply.started":"2024-02-09T17:03:35.965993Z","shell.execute_reply":"2024-02-09T17:03:35.974042Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n# NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\ndef store_submission(sent_id, tagged_sentence):\n    \n    global submission\n    submission['id'].append(sent_id)\n    submission['tagged_sentence'].append(tagged_sentence)\n    \ndef clear_submission():\n    global submission\n    submission = {'id': [], 'tagged_sentence' : []}","metadata":{"execution":{"iopub.status.busy":"2024-02-10T08:20:22.339474Z","iopub.execute_input":"2024-02-10T08:20:22.339930Z","iopub.status.idle":"2024-02-10T08:20:22.347510Z","shell.execute_reply.started":"2024-02-10T08:20:22.339895Z","shell.execute_reply":"2024-02-10T08:20:22.346094Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_id in tqdm(list(test_data.keys())):\n    sent = test_data[sent_id]\n    hmm_tagger_util(sent_id, sent)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:03:35.985063Z","iopub.execute_input":"2024-02-09T17:03:35.985434Z","iopub.status.idle":"2024-02-09T17:06:29.846893Z","shell.execute_reply.started":"2024-02-09T17:03:35.985405Z","shell.execute_reply":"2024-02-09T17:06:29.845658Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_directory = '/kaggle/working/'\npd.DataFrame(submission).to_csv(path_to_directory +' my_hmm_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T17:06:29.848735Z","iopub.execute_input":"2024-02-09T17:06:29.849169Z","iopub.status.idle":"2024-02-09T17:06:29.951353Z","shell.execute_reply.started":"2024-02-09T17:06:29.849127Z","shell.execute_reply":"2024-02-09T17:06:29.950231Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MEMM MODEL**","metadata":{"editable":false}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:14:56.265314Z","iopub.execute_input":"2024-02-10T14:14:56.265737Z","iopub.status.idle":"2024-02-10T14:14:56.272964Z","shell.execute_reply.started":"2024-02-10T14:14:56.265703Z","shell.execute_reply":"2024-02-10T14:14:56.271641Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_tags = {tag for sentence in data for word,tag in sentence}\ntag_dict = {tag: i for i, tag in enumerate(set(unique_tags))}\nfrom_num_to_tag = {}\nfor key,value in tag_dict.items():\n    from_num_to_tag[value] = key","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:14:56.473884Z","iopub.execute_input":"2024-02-10T14:14:56.474387Z","iopub.status.idle":"2024-02-10T14:14:56.568387Z","shell.execute_reply.started":"2024-02-10T14:14:56.474338Z","shell.execute_reply":"2024-02-10T14:14:56.567395Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting Features\n\nfeature_vector = []\ndef feature(sentence):\n        \n    sentence_features = []\n    for i, (word, tag) in enumerate(sentence):\n        # Get the previous word if it exists, otherwise use None\n        prev_word = sentence[i-1][0] if i > 0 else None\n\n        # Extract features for the current word\n        word_features = {\n            'capitalize': int(word[0].isupper()),\n            'digit_first': int(word[0].isdigit()),\n            's_dig__e_alpha': int(word[0].isdigit() and word[-1].isalpha()),\n            'p_anti': int(word.startswith('anti')),\n            'p_pre': int(word.startswith('pre')),\n            'p_un': int(word.startswith('un')),\n            'p_dis': int(word.startswith('dis')),\n            'p_inter': int(word.startswith('inter')),\n            'p_mis': int(word.startswith('mis')),\n            'p_non': int(word.startswith('non')),\n            'p_over_under': int(word.startswith('over') or word.startswith('under')),\n            'p_in_im': int(word.startswith('in') or word.startswith('im')),\n            'p_en_em': int(word.startswith('en') or word.startswith('em')),\n            's_able': int(word.endswith('able')),\n            's_al_ial': int(word.endswith('al') or word.endswith('ial')),\n            's_ed_ing': int(word.endswith('ed') or word.endswith('ing')),\n            's_tion_ion': int(word.endswith('tion') or word.endswith('ion')),\n            's_est': int(word.endswith('est')),\n            's_less': int(word.endswith('less')),\n            's_e_es': int(word.endswith('e') or word.endswith('es')),\n            's_en': int(word.endswith('en')),\n            's_ly': int(word.endswith('ly')),\n            's_er': int(word.endswith('er')),\n            's_\\'s_s\\'': int(word.endswith('\\'s') or word.endswith('s\\'')),\n            'prev_word_det': int(prev_word in ('a', 'an', 'the') if prev_word else 0),\n            'prev_word_det': int(prev_word in stop_words if prev_word else 0),\n\n        }\n\n        temp = []\n        for key,value in word_features.items():\n            temp.append(value);\n\n        # Add the feature dictionary to the list of features for this sentence\n        sentence_features.append(temp)\n\n    return sentence_features\n\n\nword_vector = []\ntag_vector = []\nY_train = []\n\nfor sent in data:\n    sent_feature = feature(sent)\n    for l in sent_feature:\n        feature_vector.append(l)\n        \nfor sentence in data:\n    for word,tag in sentence:\n        word_vector.append(word)\n        Y_train.append(tag_dict[tag])","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:15:42.333530Z","iopub.execute_input":"2024-02-10T14:15:42.333931Z","iopub.status.idle":"2024-02-10T14:15:56.397088Z","shell.execute_reply.started":"2024-02-10T14:15:42.333899Z","shell.execute_reply":"2024-02-10T14:15:56.395723Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(word_vector[:3])\nprint(Y_train[:3])\nprint(len(feature_vector))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:15:56.398900Z","iopub.execute_input":"2024-02-10T14:15:56.399249Z","iopub.status.idle":"2024-02-10T14:15:56.405852Z","shell.execute_reply.started":"2024-02-10T14:15:56.399220Z","shell.execute_reply":"2024-02-10T14:15:56.404575Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Logistic Regression**","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nY = np.array(Y_train)\nsoftmax_2 = LogisticRegression( multi_class = 'multinomial', solver = 'lbfgs',max_iter = 100,penalty = 'l2')\nsoftmax_2.fit(feature_vector,Y)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:15:56.407055Z","iopub.execute_input":"2024-02-10T14:15:56.407437Z","iopub.status.idle":"2024-02-10T14:18:30.322705Z","shell.execute_reply.started":"2024-02-10T14:15:56.407404Z","shell.execute_reply":"2024-02-10T14:18:30.321385Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_vector = np.array(feature_vector)\nsoftmax_2.predict_proba(feature_vector[0].reshape(1,-1))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T08:11:32.758118Z","iopub.execute_input":"2024-02-10T08:11:32.758804Z","iopub.status.idle":"2024-02-10T08:11:33.992700Z","shell.execute_reply.started":"2024-02-10T08:11:32.758755Z","shell.execute_reply":"2024-02-10T08:11:33.991806Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def viterbi_algorithm_memm(sentence):\n    n = len(sentence)\n    m = len(states)\n    \n    dp = np.zeros((n, m))\n    backpointer = np.zeros((n, m), dtype=int)\n    \n    \n    # Initialization step\n    to_pass = [(word,'_') for word in sentence]\n    word_feature = feature(to_pass)\n    temp = word_feature[0]\n    temp = np.array(temp)\n    \n    probs_start = softmax_2.predict_proba(temp.reshape(1,-1))\n    \n    for i in range(m):\n        dp[0][i] = probs_start[0][i]\n        \n    # Recursion step\n    for t in range(1, n):\n        temp = word_feature[t]\n        temp = np.array(temp)\n        probs_new = softmax_2.predict_proba(temp.reshape(1,-1))\n        for j in range(m):\n            word = sentence[t]\n            max_prob = 0\n            max_index = 0\n            \n            for i in range(m):\n                prob = dp[t-1][i] * probs_new[0][j]\n\n                if prob > max_prob:\n                    max_prob = prob\n                    max_index = i\n            \n            dp[t][j] = max_prob\n            backpointer[t][j] = max_index\n\n    # Backtrack to find the best sequence of tags\n    best_sequence = [0] * n\n    best_index = np.argmax(dp[n-1, :])\n\n    for t in range(n-1, -1, -1):\n        best_sequence[t] = states[best_index]\n        best_index = backpointer[t][best_index]\n\n    return best_sequence","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:24:01.464173Z","iopub.execute_input":"2024-02-10T14:24:01.464629Z","iopub.status.idle":"2024-02-10T14:24:01.478300Z","shell.execute_reply.started":"2024-02-10T14:24:01.464592Z","shell.execute_reply":"2024-02-10T14:24:01.477232Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def memm_tagger_util(sent_id, untagged_sentence):\n#     tagged_sentence = []\n#     predicted_tag = []\n#     to_pass = [(word,'_') for word in untagged_sentence]\n#     word_feature = feature(to_pass)\n#     for temp in word_feature:\n#         temp = np.array(temp)\n#         temp = temp.reshape(1,-1)\n        \n#         predicted = softmax_2.predict(temp)\n#         tag = from_num_to_tag[predicted[0]]\n#         predicted_tag.append(tag)\n        \n#     for index ,word in enumerate(list(untagged_sentence)):\n#         tagged_sentence.append((word, predicted_tag[index])) \n    \n#     print(tagged_sentence)\n    predicted_tags = viterbi_algorithm_memm(untagged_sentence)\n    tagged_sentence = []\n    for index ,word in enumerate(list(untagged_sentence)):\n        tagged_sentence.append((word, predicted_tags[index])) \n#     print(tagged_sentence)\n    store_submission(sent_id, tagged_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:24:01.596953Z","iopub.execute_input":"2024-02-10T14:24:01.597460Z","iopub.status.idle":"2024-02-10T14:24:01.604141Z","shell.execute_reply.started":"2024-02-10T14:24:01.597421Z","shell.execute_reply":"2024-02-10T14:24:01.603199Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_id in tqdm(list(test_data.keys())):\n    sent = test_data[sent_id]\n    memm_tagger_util(sent_id, sent)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T14:24:02.062461Z","iopub.execute_input":"2024-02-10T14:24:02.062884Z","iopub.status.idle":"2024-02-10T14:24:04.073017Z","shell.execute_reply.started":"2024-02-10T14:24:02.062848Z","shell.execute_reply":"2024-02-10T14:24:04.071460Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_directory = '/kaggle/working/'\npd.DataFrame(submission).to_csv(path_to_directory +' wwww_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T08:20:34.572876Z","iopub.execute_input":"2024-02-10T08:20:34.573127Z","iopub.status.idle":"2024-02-10T08:20:34.636084Z","shell.execute_reply.started":"2024-02-10T08:20:34.573104Z","shell.execute_reply":"2024-02-10T08:20:34.635080Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}